<h2 align="center"> <a href="https://arxiv.org/pdf/2505.18110">ðŸ¤– [NeurIPS 2025] Watch and Listen: Understanding Audio-Visual-Speech Moments with Multimodal LLM</a></h2>
<h5 align="center"> Zinuo Li<sup>1</sup>, Xian Zhang<sup>1</sup>, Yongxin Guo<sup>2</sup>, Mohammed Bennamoun<sup>1</sup>, Farid Boussaid<sup>1</sup>, Girish Dwivedi<sup>1</sup>, Luqi Gong<sup>3</sup>, Qiuhong Ke<sup>4</sup> </h5>
<h5 align="center">  <sup>1</sup>University of Western Australia, <sup>2</sup>Alibaba Group, <sup>3</sup>Zhejiang Laboratory, <sup>4</sup>Monash University </h5>

<h5 align="center">
[![hf_data](https://img.shields.io/badge/ðŸ¤—-Datasets-9C276A.svg)](https://huggingface.co/datasets/zinuoli/TriSense-2M)
[![arxiv](https://img.shields.io/badge/Arxiv-2410.05643-b31b1b.svg?logo=arXiv)](https://arxiv.org/pdf/2505.18110?)

![af355688748a212818b4746797e6731](https://github.com/user-attachments/assets/fbc89818-b878-4efe-b72c-959f35db169e)

## ðŸ“¢ TODO List
This repo is under construction. We are currently busy with paper & dataset $ codes finalization.

We will continuously update this repo for better contribution and performance.
- [x] Release dataset
- [x] Higher quality data
- [ ] Release trainining code
- [ ] Post-training on highher quality data
