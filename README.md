<h1 align="center"> <a href="https://arxiv.org/pdf/2505.18110">Watch and Listen: Understanding Audio-Visual-Speech Moments with Multimodal LLM</a></h1>
<h5 align="center"> Zinuo Li<sup> 1 </sup>, Xian Zhang<sup> 1 </sup>, Yongxin Guo<sup> 2 </sup>, Mohammed Bennamoun<sup> 1 </sup>, Farid Boussaid<sup> 1 </sup>, Girish Dwivedi<sup> 1 </sup>, Luqi Gong<sup> 3 </sup>, Qiuhong Ke<sup> 4 </sup> </h5>
<h5 align="center">  <sup> 1 </sup>University of Western Australia, <sup> 2 </sup>Alibaba Group, <sup> 3 </sup>Zhejiang Laboratory, <sup> 4 </sup>Monash University </h5>

![af355688748a212818b4746797e6731](https://github.com/user-attachments/assets/fbc89818-b878-4efe-b72c-959f35db169e)

## TODO List
- [ ] Release dataset
- [ ] Release checkpoints & inference code
- [ ] Release trainining code
- [ ] Huggingface demo
- [ ] Post-training on highher quality and more data
